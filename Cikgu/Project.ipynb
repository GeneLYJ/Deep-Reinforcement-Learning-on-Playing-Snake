{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from game import SnakeGameAI, Direction, Point\n",
    "from model import Linear_QNet, QTrainer\n",
    "from helper import plot\n",
    "\n",
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 1000\n",
    "LR = 0.001\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = 0 # randomness\n",
    "        self.gamma = 0.9 # discount rate\n",
    "        self.memory = deque(maxlen=MAX_MEMORY) # popleft()\n",
    "        self.model = Linear_QNet(11, 256, 3)\n",
    "        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)\n",
    "\n",
    "\n",
    "    def get_state(self, game):\n",
    "        head = game.snake[0]\n",
    "        west = Point(head.x - 20, head.y)\n",
    "        east = Point(head.x + 20, head.y)\n",
    "        point_u = Point(head.x, head.y - 20)\n",
    "        point_d = Point(head.x, head.y + 20)\n",
    "        \n",
    "        dir_l = game.direction == Direction.LEFT\n",
    "        dir_r = game.direction == Direction.RIGHT\n",
    "        dir_u = game.direction == Direction.UP\n",
    "        dir_d = game.direction == Direction.DOWN\n",
    "\n",
    "        state = [\n",
    "            # Danger straight\n",
    "            (dir_r and game.is_collision(east)) or \n",
    "            (dir_l and game.is_collision(west)) or \n",
    "            (dir_u and game.is_collision(point_u)) or \n",
    "            (dir_d and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger right\n",
    "            (dir_u and game.is_collision(east)) or \n",
    "            (dir_d and game.is_collision(west)) or \n",
    "            (dir_l and game.is_collision(point_u)) or \n",
    "            (dir_r and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger left\n",
    "            (dir_d and game.is_collision(east)) or \n",
    "            (dir_u and game.is_collision(west)) or \n",
    "            (dir_r and game.is_collision(point_u)) or \n",
    "            (dir_l and game.is_collision(point_d)),\n",
    "            \n",
    "            # Move direction\n",
    "            dir_l,\n",
    "            dir_r,\n",
    "            dir_u,\n",
    "            dir_d,\n",
    "            \n",
    "            # Food location \n",
    "            game.food.x < game.head.x,  # food left\n",
    "            game.food.x > game.head.x,  # food right\n",
    "            game.food.y < game.head.y,  # food up\n",
    "            game.food.y > game.head.y  # food down\n",
    "            ]\n",
    "\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # popleft if MAX_MEMORY is reached\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "        #for state, action, reward, nexrt_state, done in mini_sample:\n",
    "        #    self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # random moves: tradeoff exploration / exploitation\n",
    "        self.epsilon = 80 - self.n_games\n",
    "        final_move = [0,0,0]\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            move = random.randint(0, 2)\n",
    "            final_move[move] = 1\n",
    "        else:\n",
    "            state0 = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state0)\n",
    "            move = torch.argmax(prediction).item()\n",
    "            final_move[move] = 1\n",
    "\n",
    "        return final_move\n",
    "\n",
    "\n",
    "def train():\n",
    "    plot_scores = []\n",
    "    plot_mean_scores = []\n",
    "    total_score = 0\n",
    "    record = 0\n",
    "    agent = Agent()\n",
    "    game = SnakeGameAI()\n",
    "    while True:\n",
    "        # get old state\n",
    "        state_old = agent.get_state(game)\n",
    "\n",
    "        # get move\n",
    "        final_move = agent.get_action(state_old)\n",
    "\n",
    "        # perform move and get new state\n",
    "        reward, done, score = game.play_step(final_move)\n",
    "        state_new = agent.get_state(game)\n",
    "\n",
    "        # train short memory\n",
    "        agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        # remember\n",
    "        agent.remember(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        if done:\n",
    "            # train long memory, plot result\n",
    "            game.reset()\n",
    "            agent.n_games += 1\n",
    "            agent.train_long_memory()\n",
    "\n",
    "            if score > record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "\n",
    "            print('Game', agent.n_games, 'Score', score, 'Record:', record)\n",
    "\n",
    "            plot_scores.append(score)\n",
    "            total_score += score\n",
    "            mean_score = total_score / agent.n_games\n",
    "            plot_mean_scores.append(mean_score)\n",
    "            plot(plot_scores, plot_mean_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Point(x=300.0, y=120.0) Point(x=340.0, y=120.0) Point(x=320.0, y=100.0) Point(x=320.0, y=140.0)\n",
      "False False True False\n",
      "False False False False False True False False True False True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from game import SnakeGameAI, Direction, Point\n",
    "from helper import plot\n",
    "import pygame\n",
    "import os\n",
    "\n",
    "import math, random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set device to GPU_indx if GPU is avaliable\n",
    "GPU_indx = 0\n",
    "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state(game):\n",
    "    head = game.snake[0]\n",
    "    west = Point(head.x - 20, head.y)\n",
    "    east = Point(head.x + 20, head.y)\n",
    "    north = Point(head.x, head.y - 20)\n",
    "    south = Point(head.x, head.y + 20)\n",
    "\n",
    "    left = game.direction == Direction.LEFT\n",
    "    right = game.direction == Direction.RIGHT\n",
    "    up = game.direction == Direction.UP\n",
    "    down = game.direction == Direction.DOWN\n",
    "    #print(game.snake)\n",
    "    #print(left, right, up, down)\n",
    "    \n",
    "    state = [\n",
    "        # Detect collision when moving straight\n",
    "        (right and game.is_collision(east)) or \n",
    "        (left and game.is_collision(west)) or \n",
    "        (up and game.is_collision(north)) or \n",
    "        (down and game.is_collision(south)),\n",
    "\n",
    "        # Detect collision when moving right\n",
    "        (up and game.is_collision(east)) or \n",
    "        (down and game.is_collision(west)) or \n",
    "        (left and game.is_collision(north)) or \n",
    "        (right and game.is_collision(south)),\n",
    "\n",
    "        # Detect collision when moving left\n",
    "        (down and game.is_collision(east)) or \n",
    "        (up and game.is_collision(west)) or \n",
    "        (right and game.is_collision(north)) or \n",
    "        (left and game.is_collision(south)),\n",
    "\n",
    "        # Move direction\n",
    "        left, right, up, down,\n",
    "\n",
    "        # Food location \n",
    "        game.food.x < game.head.x,  # food left\n",
    "        game.food.x > game.head.x,  # food right\n",
    "        game.food.y < game.head.y,  # food up\n",
    "        game.food.y > game.head.y  # food down\n",
    "        ]\n",
    "    #print(state)\n",
    "    return np.array(state, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity) # popleft()\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # popleft if MAX_MEMORY is reached\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.memory, batch_size))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_array(action, num_actions):\n",
    "    if (action < num_actions):\n",
    "        move = np.zeros(num_actions).astype(np.int)\n",
    "        move[action] = 1\n",
    "        #move = move.tolist()\n",
    "        return move.tolist()\n",
    "    else:\n",
    "        print(\"exceed number of actions\\n\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(state, action, reward, next_state, done):\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.long)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)\n",
    "    #done = torch.FloatTensor(done)\n",
    "    # (n, x)\n",
    "    \n",
    "    if len(state.shape) == 1:\n",
    "        # (1, x)\n",
    "        state = torch.unsqueeze(state, 0)\n",
    "        next_state = torch.unsqueeze(next_state, 0)\n",
    "        action = torch.unsqueeze(action, 0)\n",
    "        reward = torch.unsqueeze(reward, 0)\n",
    "        done = (done, )\n",
    "\n",
    "    # 1: predicted Q values with current state\n",
    "    Q_i = model(state.to(device))\n",
    "\n",
    "    target_Q = Q_i.clone()\n",
    "    for idx in range(len(done)):\n",
    "        Q_new = reward[idx]\n",
    "        if not done[idx]:\n",
    "            Q_new = reward[idx].to(device) + gamma.to(device) * torch.max(model(next_state[idx].to(device)))\n",
    "\n",
    "        target_Q[idx][torch.argmax(action[idx]).item()] = Q_new\n",
    "\n",
    "    # 2: Q_new = r + y * max(next_predicted Q value) -> only do this if not done\n",
    "    # pred.clone()\n",
    "    # preds[argmax(action)] = Q_new\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(Q_i, target) #!!\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1\n",
    "epsilon_final = 0.0001\n",
    "epsilon_decay = 10000\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_input = 3\n",
    "num_actions = 3\n",
    "model = CnnDQN(channels_input, num_actions).to(device) # (180, 120, 3) , (6)\n",
    "\n",
    "lr = 5e-5\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "replay_initial = 500\n",
    "replay_buffer = ReplayBuffer(6000)\n",
    "\n",
    "# Frame:\n",
    "start = 1\n",
    "total_frames = 50000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "# start:\n",
    "game = SnakeGameAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    plot_scores = []\n",
    "    plot_mean_scores = []\n",
    "    total_score = 0\n",
    "    record = 0\n",
    "    agent = Agent()\n",
    "    #game = SnakeGameAI()\n",
    "    \n",
    "    move = np.zeros(3).astype(np.int)\n",
    "    move = move.tolist()\n",
    "    game.play_step(move)\n",
    "    state_i = state(game)\n",
    "    while True:\n",
    "        model.train()\n",
    "        # get old state (done)\n",
    "        \n",
    "        # get move \n",
    "        #final_move = agent.get_action(state_i)\n",
    "        \n",
    "        #epsilon = epsilon_by_frame(episode) # No. 1\n",
    "        if random.random() > epsilon:\n",
    "            x = torch.tensor(state_i, dtype=torch.float)\n",
    "            q_value = model(x.to(device))\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "            #state = state.squeeze(0)\n",
    "        else:\n",
    "            action = random.randrange(num_actions)\n",
    "\n",
    "        move = action_array(action)\n",
    "\n",
    "        # perform move and get new state\n",
    "        reward, done, score = game.play_step(final_move) # No.2\n",
    "        state_i_new = state(game)\n",
    "\n",
    "        # train short memory\n",
    "        loss = Loss(state_i, move, reward, state_i_new, done)\n",
    "\n",
    "        # push\n",
    "        replay_buffer.push(state_i, final_move, reward, state_i_new, done)\n",
    "        state_i = state_i_new\n",
    "        #episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            # train long memory, plot result\n",
    "            game.reset()\n",
    "            episode += 1\n",
    "            \n",
    "            if len(replay_buffer) > batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "            else:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer\n",
    "\n",
    "            loss = Loss(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            if score > record:\n",
    "                record = score\n",
    "                save(model)\n",
    "\n",
    "            print('Game', agent.n_games, 'Score', score, 'Record:', record)\n",
    "\n",
    "            plot_scores.append(score)\n",
    "            total_score += score\n",
    "            mean_score = total_score / agent.n_games\n",
    "            plot_mean_scores.append(mean_score)\n",
    "            plot(plot_scores, plot_mean_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
